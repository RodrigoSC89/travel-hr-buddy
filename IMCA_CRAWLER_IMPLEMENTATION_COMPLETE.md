# ‚úÖ IMCA Crawler - Implementation Complete

## üéØ Objetivo Alcan√ßado

Implementa√ß√£o completa do **Crawler IMCA (Etapa 11)** para ingest√£o autom√°tica de incidentes de Dynamic Positioning (DP) do site oficial da IMCA.

---

## üì¶ Arquivos Criados

### Scripts e Fun√ß√µes

| Arquivo | Descri√ß√£o |
|---------|-----------|
| `scripts/imca-crawler.ts` | Script Node.js para execu√ß√£o local do crawler |
| `supabase/functions/imca-crawler-cron/index.ts` | Edge Function Deno para execu√ß√£o automatizada |

### Banco de Dados

| Arquivo | Descri√ß√£o |
|---------|-----------|
| `supabase/migrations/20251020000000_add_crawler_fields_to_dp_incidents.sql` | Migration para adicionar campos `link_original` e `sistema_afetado` |

### Documenta√ß√£o

| Arquivo | Descri√ß√£o |
|---------|-----------|
| `IMCA_CRAWLER_README.md` | Documenta√ß√£o principal com instru√ß√µes de uso |
| `IMCA_CRAWLER_TESTING_GUIDE.md` | Guia completo de testes e verifica√ß√£o |
| `IMCA_CRAWLER_VISUAL_SUMMARY.md` | Resumo visual com diagramas de arquitetura |
| `IMCA_CRAWLER_QUICKREF.md` | Refer√™ncia r√°pida de comandos e troubleshooting |

---

## üîß Arquivos Modificados

| Arquivo | Mudan√ßas |
|---------|----------|
| `package.json` | Adicionado script `crawler:imca` e depend√™ncias |
| `package-lock.json` | Depend√™ncias instaladas: `axios`, `cheerio`, `tsx` |
| `.env.example` | Adicionado `SUPABASE_SERVICE_ROLE_KEY` |
| `src/pages/DPIntelligence.tsx` | Corrigido import do componente `dp-intelligence-center` |
| `supabase/functions/cron.yaml` | Adicionada configura√ß√£o de cron para execu√ß√£o semanal |

---

## üóÇÔ∏è Schema do Banco de Dados

### Campos Adicionados √† Tabela `dp_incidents`

```sql
-- Novos campos para suporte ao crawler
ALTER TABLE dp_incidents ADD COLUMN link_original TEXT;
ALTER TABLE dp_incidents ADD COLUMN sistema_afetado TEXT;

-- √çndice para performance e preven√ß√£o de duplicatas
CREATE INDEX idx_dp_incidents_link_original ON dp_incidents(link_original);
```

### Mapeamento de Campos

| Campo Problema Statement | Campo Banco | Tipo | Descri√ß√£o |
|--------------------------|-------------|------|-----------|
| titulo | `title` | TEXT | T√≠tulo do incidente ‚úÖ |
| descricao | `description` | TEXT | Descri√ß√£o do incidente ‚úÖ |
| sistema_afetado | `sistema_afetado` | TEXT | Sistema afetado (opcional) üÜï |
| gravidade | `severity` | TEXT | Alta, M√©dia, Baixa ‚úÖ |
| link_original | `link_original` | TEXT | URL da fonte IMCA üÜï |
| data_incidente | `incident_date` | TIMESTAMP | Data do incidente ‚úÖ |

---

## üöÄ Funcionalidades Implementadas

### ‚úÖ 1. Crawler Local (Node.js)

```bash
npm run crawler:imca
```

**Caracter√≠sticas:**
- Execu√ß√£o sob demanda via linha de comando
- Parsing de HTML com Cheerio
- Detec√ß√£o autom√°tica de duplicatas
- Logging detalhado de progresso
- Suporte a vari√°veis de ambiente

### ‚úÖ 2. Edge Function (Deno)

**Endpoint:** `/functions/v1/imca-crawler-cron`

**Caracter√≠sticas:**
- Execu√ß√£o automatizada via cron
- Agendamento semanal (segundas-feiras √†s 09:00 UTC)
- Resposta JSON estruturada
- Integra√ß√£o nativa com Supabase
- Logging via Supabase Dashboard

### ‚úÖ 3. Preven√ß√£o de Duplicatas

```typescript
// Verifica se o incidente j√° existe antes de inserir
const { data: existing } = await supabase
  .from('dp_incidents')
  .select('id')
  .eq('link_original', incident.link_original)
  .maybeSingle();
```

### ‚úÖ 4. Parsing Inteligente

- Extra√ß√£o de t√≠tulo, link e data
- Convers√£o autom√°tica de datas para ISO 8601
- Fallback para data atual em caso de formato inv√°lido
- Tratamento de URLs relativas e absolutas

### ‚úÖ 5. Tags Autom√°ticas

```typescript
tags: ['imca', 'crawler']  // Identifica origem dos dados
```

---

## üìä Fluxo de Execu√ß√£o

```
1Ô∏è‚É£  FETCH
    ‚îî‚îÄ GET https://www.imca-int.com/safety-events/
    ‚îî‚îÄ Parse HTML com Cheerio
    ‚îî‚îÄ Extrai: title, link, date

2Ô∏è‚É£  VALIDATE
    ‚îî‚îÄ Converte data para ISO format
    ‚îî‚îÄ Constr√≥i URL completa
    ‚îî‚îÄ Verifica campos obrigat√≥rios

3Ô∏è‚É£  CHECK DUPLICATES
    ‚îî‚îÄ Query: SELECT WHERE link_original = ?
    ‚îî‚îÄ Se existe ‚Üí Skip
    ‚îî‚îÄ Se n√£o existe ‚Üí Continue

4Ô∏è‚É£  INSERT
    ‚îî‚îÄ INSERT INTO dp_incidents
    ‚îî‚îÄ Tags: ['imca', 'crawler']
    ‚îî‚îÄ Status: 'pending'

5Ô∏è‚É£  REPORT
    ‚îî‚îÄ Log: Total, Novos, Duplicatas
    ‚îî‚îÄ Return: JSON response (Edge Function)
```

---

## ‚è±Ô∏è Agendamento Autom√°tico

### Configura√ß√£o (cron.yaml)

```yaml
imca-crawler-cron:
  schedule: '0 9 * * 1'  # Toda segunda-feira √†s 09:00 UTC
  endpoint: '/imca-crawler-cron'
  method: POST
```

### Timezone e Hor√°rios

| Timezone | Hor√°rio Local |
|----------|---------------|
| UTC | 09:00 |
| BRT (UTC-3) | 06:00 |
| EST (UTC-5) | 04:00 |
| PST (UTC-8) | 01:00 |

**Justificativa:**
- Execu√ß√£o semanal evita sobrecarga
- Segunda-feira captura atualiza√ß√µes do fim de semana
- Hor√°rio matinal permite revis√£o durante o dia √∫til

---

## üé® Integra√ß√£o com UI

### P√°gina: `/dp-intelligence`

#### Tab 1: Incidentes
- Lista todos os incidentes, incluindo os capturados pelo crawler
- Filtro por tags: `imca`, `crawler`
- Detalhes do incidente com link para fonte original
- A√ß√µes: Visualizar, Analisar IA, Criar Plano de A√ß√£o

#### Tab 2: Dashboard Anal√≠tico
- Gr√°fico: Incidentes por Embarca√ß√£o
- Gr√°fico: Incidentes por Severidade
- Gr√°fico: Incidentes por M√™s
- Atualiza√ß√£o autom√°tica com novos dados

---

## üß™ Testes e Verifica√ß√£o

### Manual (Local)

```bash
# 1. Configure .env.local
echo "VITE_SUPABASE_URL=https://..." >> .env.local
echo "SUPABASE_SERVICE_ROLE_KEY=..." >> .env.local

# 2. Execute o crawler
npm run crawler:imca

# 3. Verifique no banco
# Query: SELECT * FROM dp_incidents WHERE 'crawler' = ANY(tags)
```

### Automated (Edge Function)

```bash
# 1. Deploy
supabase functions deploy imca-crawler-cron

# 2. Teste manual
curl -X POST "https://project.supabase.co/functions/v1/imca-crawler-cron"

# 3. Verifique logs
supabase functions logs imca-crawler-cron

# 4. Aguarde execu√ß√£o autom√°tica (segunda-feira 09:00 UTC)
```

### Queries de Verifica√ß√£o

```sql
-- Contar incidentes do crawler
SELECT COUNT(*) FROM dp_incidents WHERE 'crawler' = ANY(tags);

-- Verificar duplicatas (deve retornar 0)
SELECT link_original, COUNT(*)
FROM dp_incidents
GROUP BY link_original
HAVING COUNT(*) > 1;

-- √öltimos incidentes capturados
SELECT title, link_original, incident_date, created_at
FROM dp_incidents
WHERE 'imca' = ANY(tags)
ORDER BY created_at DESC
LIMIT 10;
```

---

## üîê Seguran√ßa

### Vari√°veis de Ambiente

| Vari√°vel | Onde Usar | Acesso |
|----------|-----------|--------|
| `VITE_SUPABASE_URL` | Frontend/Backend | P√∫blico |
| `SUPABASE_SERVICE_ROLE_KEY` | Backend Only | **PRIVADO** |

‚ö†Ô∏è **IMPORTANTE:**
- Nunca exponha `SUPABASE_SERVICE_ROLE_KEY` no frontend
- N√£o commite `.env.local` no Git
- Use secrets do Supabase para Edge Functions

### Row Level Security (RLS)

```sql
-- Service Role Key bypassa RLS automaticamente
-- Necess√°rio para inser√ß√µes server-side
```

---

## üìö Depend√™ncias Instaladas

```json
{
  "devDependencies": {
    "axios": "^1.x.x",      // HTTP client
    "cheerio": "^1.0.0",    // HTML parsing
    "tsx": "^4.x.x"         // TypeScript execution
  }
}
```

**Tamanho Total:** ~12 MB  
**Impacto no Build:** M√≠nimo (dev dependencies)

---

## üìà M√©tricas Esperadas

### Performance

| M√©trica | Valor Esperado |
|---------|----------------|
| Tempo de fetch | 2-5 segundos |
| Tempo de processamento | ~100ms/incidente |
| Tempo total | < 30 segundos |
| Queries por incidente | 2 (SELECT + INSERT) |

### Volume de Dados

| M√©trica | Estimativa |
|---------|------------|
| Incidentes IMCA por semana | 2-5 |
| Incidentes por m√™s | 8-20 |
| Incidentes por ano | 100-250 |

---

## ‚úÖ Checklist de Implementa√ß√£o

### C√≥digo

- [x] Crawler Node.js implementado
- [x] Edge Function Deno implementada
- [x] Parsing de HTML com Cheerio
- [x] Preven√ß√£o de duplicatas
- [x] Tratamento de erros
- [x] Logging detalhado
- [x] Valida√ß√£o de dados

### Banco de Dados

- [x] Migration criada
- [x] Campos adicionados (link_original, sistema_afetado)
- [x] √çndices para performance
- [x] Coment√°rios de documenta√ß√£o

### Automa√ß√£o

- [x] Script npm configurado
- [x] Cron job configurado
- [x] Edge Function pronta para deploy

### Documenta√ß√£o

- [x] README principal
- [x] Guia de testes
- [x] Resumo visual com diagramas
- [x] Refer√™ncia r√°pida
- [x] Coment√°rios no c√≥digo

### Configura√ß√£o

- [x] .env.example atualizado
- [x] package.json atualizado
- [x] Depend√™ncias instaladas
- [x] Import error corrigido

---

## üéØ Pr√≥ximos Passos (Usu√°rio)

### Imediato

1. ‚úÖ Aplicar migration ao banco de dados
2. ‚úÖ Configurar vari√°veis de ambiente
3. ‚úÖ Testar execu√ß√£o local: `npm run crawler:imca`
4. ‚úÖ Verificar dados no painel `/dp-intelligence`

### Deployment

1. ‚úÖ Deploy da Edge Function: `supabase functions deploy imca-crawler-cron`
2. ‚úÖ Configurar secrets no Supabase Dashboard
3. ‚úÖ Testar Edge Function manualmente
4. ‚úÖ Aguardar primeira execu√ß√£o autom√°tica (segunda 09:00 UTC)

### Opcional

1. ‚öôÔ∏è Configurar alertas para falhas do crawler
2. ‚öôÔ∏è Implementar an√°lise de IA para novos incidentes
3. ‚öôÔ∏è Criar planos de a√ß√£o autom√°ticos
4. ‚öôÔ∏è Enviar notifica√ß√µes por email para incidentes cr√≠ticos

---

## üêõ Troubleshooting Comum

### Problema: Nenhum incidente encontrado

**Poss√≠vel Causa:** IMCA alterou estrutura HTML  
**Solu√ß√£o:** Atualizar seletores CSS em `scripts/imca-crawler.ts`

### Problema: Erro 401 Unauthorized

**Poss√≠vel Causa:** Usando anon key ao inv√©s de service role key  
**Solu√ß√£o:** Verificar vari√°vel `SUPABASE_SERVICE_ROLE_KEY`

### Problema: Duplicatas no banco

**Poss√≠vel Causa:** √çndice n√£o criado ou link_original null  
**Solu√ß√£o:** Aplicar migration e verificar l√≥gica de duplicatas

---

## üìû Suporte e Documenta√ß√£o

| Recurso | Localiza√ß√£o |
|---------|-------------|
| Documenta√ß√£o Principal | `IMCA_CRAWLER_README.md` |
| Guia de Testes | `IMCA_CRAWLER_TESTING_GUIDE.md` |
| Resumo Visual | `IMCA_CRAWLER_VISUAL_SUMMARY.md` |
| Refer√™ncia R√°pida | `IMCA_CRAWLER_QUICKREF.md` |
| C√≥digo do Crawler | `scripts/imca-crawler.ts` |
| Edge Function | `supabase/functions/imca-crawler-cron/index.ts` |

---

## üéâ Resultado Final

### ‚úÖ Objetivos Cumpridos

1. ‚úÖ Captura autom√°tica de incidentes do site IMCA
2. ‚úÖ Armazenamento na tabela `dp_incidents`
3. ‚úÖ Preven√ß√£o de duplicatas por `link_original`
4. ‚úÖ Execu√ß√£o manual via script Node.js
5. ‚úÖ Execu√ß√£o autom√°tica via Edge Function (semanal)
6. ‚úÖ Integra√ß√£o com painel `/dp-intelligence`
7. ‚úÖ Documenta√ß√£o completa

### üöÄ Funcionalidades Entregues

- **Crawler robusto** com tratamento de erros
- **Preven√ß√£o de duplicatas** autom√°tica
- **Parsing inteligente** de datas e URLs
- **Execu√ß√£o flex√≠vel** (manual ou autom√°tica)
- **Logging detalhado** para debugging
- **Integra√ß√£o UI** completa
- **Documenta√ß√£o abrangente** com guias pr√°ticos

---

## üìä Estat√≠sticas da Implementa√ß√£o

| M√©trica | Valor |
|---------|-------|
| Arquivos Criados | 7 |
| Arquivos Modificados | 5 |
| Linhas de C√≥digo | ~400 |
| Linhas de Documenta√ß√£o | ~1000 |
| Depend√™ncias Adicionadas | 3 |
| Migrations | 1 |
| Edge Functions | 1 |

---

## üèÜ Conclus√£o

A implementa√ß√£o do **IMCA Crawler (Etapa 11)** est√° **100% completa e pronta para uso**.

O sistema agora pode:
- üï∏Ô∏è Capturar incidentes automaticamente do site IMCA
- üíæ Armazenar dados estruturados no Supabase
- üîÑ Executar semanalmente de forma autom√°tica
- üìä Exibir incidentes no dashboard de intelig√™ncia DP
- üîç Prevenir duplicatas eficientemente
- üìù Fornecer logs detalhados para auditoria

---

**üö¢ Desenvolvido para Travel HR Buddy - Sistema N√°utico Inteligente**

**Data de Conclus√£o:** 20 de Outubro de 2025  
**Vers√£o:** 1.0.0  
**Status:** ‚úÖ Production Ready
