# ğŸ•¸ï¸ IMCA Crawler - Etapa 11

## ğŸ¯ Objetivo

Capturar automaticamente os Ãºltimos incidentes pÃºblicos do site oficial da IMCA:

ğŸ”— https://www.imca-int.com/safety-events/

E salvar no Supabase na tabela `dp_incidents`.

## ğŸ“‹ Campos da Tabela `dp_incidents`

| Campo | Tipo | DescriÃ§Ã£o |
|-------|------|-----------|
| titulo (title) | text | TÃ­tulo do incidente |
| descricao (description) | text | DescriÃ§Ã£o extraÃ­da da pÃ¡gina |
| sistema_afetado | text | (Opcional) detectado via NLP ou padrÃ£o textual |
| gravidade (severity) | text | Alta, MÃ©dia ou Baixa |
| link_original | text | URL completa da fonte original |
| data_incidente (incident_date) | date | Data de publicaÃ§Ã£o do evento |

## ğŸš€ Como Executar

### 1. Configurar variÃ¡veis de ambiente

Certifique-se de que o arquivo `.env.local` contÃ©m:

```bash
VITE_SUPABASE_URL=https://seu-projeto.supabase.co
SUPABASE_SERVICE_ROLE_KEY=eyJhbGc...seu-service-role-key
```

> âš ï¸ **Importante**: O `SUPABASE_SERVICE_ROLE_KEY` Ã© necessÃ¡rio para operaÃ§Ãµes server-side e bypass de RLS (Row Level Security).

### 2. Executar o crawler

```bash
npm run crawler:imca
```

Ou diretamente:

```bash
npx tsx scripts/imca-crawler.ts
```

### 3. Resultado Esperado

O crawler irÃ¡:

1. âœ… Acessar https://www.imca-int.com/safety-events/
2. âœ… Extrair informaÃ§Ãµes dos incidentes listados
3. âœ… Verificar duplicatas por `link_original`
4. âœ… Salvar apenas incidentes novos no Supabase
5. âœ… Exibir resumo da execuÃ§Ã£o

Exemplo de saÃ­da:

```
ğŸš€ Starting IMCA Crawler...

ğŸŒ Fetching IMCA safety events from: https://www.imca-int.com/safety-events/
âœ… Found 15 incidents on IMCA website

ğŸ†• New incident saved: Loss of Position Due to Gyro Drift
â­ï¸  Already exists: Thruster Control Software Failure
ğŸ†• New incident saved: Reference System Failure in Heavy Weather
...

ğŸ“Š Summary:
   Total incidents found: 15
   New incidents saved: 8
   Duplicates skipped: 7

âœ… IMCA Crawler completed successfully!
```

## ğŸ“Š Visualizar os Dados

ApÃ³s a execuÃ§Ã£o do crawler, os novos incidentes estarÃ£o disponÃ­veis em:

ğŸ”— **Painel**: `/dp-intelligence`

Os incidentes aparecem automaticamente na:
- Tab "Incidentes" - Lista completa de incidentes
- Tab "Dashboard AnalÃ­tico" - EstatÃ­sticas e grÃ¡ficos

## âš™ï¸ Executar Periodicamente (Opcional)

Para executar o crawler automaticamente 1x por semana:

### OpÃ§Ã£o 1: Supabase Edge Function com Cron âœ… (Recomendado)

Uma Edge Function jÃ¡ estÃ¡ configurada em:
- **FunÃ§Ã£o**: `supabase/functions/imca-crawler-cron/index.ts`
- **Cron**: Todo segunda-feira Ã s 09:00 UTC (definido em `supabase/functions/cron.yaml`)

Para testar manualmente:

```bash
# Via Supabase CLI
supabase functions serve imca-crawler-cron

# Via curl (se a funÃ§Ã£o jÃ¡ estiver deployed)
curl -X POST https://seu-projeto.supabase.co/functions/v1/imca-crawler-cron \
  -H "Authorization: Bearer seu-service-role-key"
```

Para fazer deploy da Edge Function:

```bash
supabase functions deploy imca-crawler-cron
```

A funÃ§Ã£o executarÃ¡ automaticamente toda segunda-feira Ã s 09:00 UTC conforme configurado no `cron.yaml`.

### OpÃ§Ã£o 2: GitHub Actions (Workflow)

Crie um workflow em `.github/workflows/imca-crawler.yml`:

```yaml
name: IMCA Crawler

on:
  schedule:
    - cron: '0 9 * * 1' # Every Monday at 9:00 AM UTC
  workflow_dispatch: # Allow manual trigger

jobs:
  crawler:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
      - uses: actions/setup-node@v4
        with:
          node-version: '22'
      - run: npm install
      - run: npm run crawler:imca
        env:
          VITE_SUPABASE_URL: ${{ secrets.VITE_SUPABASE_URL }}
          SUPABASE_SERVICE_ROLE_KEY: ${{ secrets.SUPABASE_SERVICE_ROLE_KEY }}
```

### OpÃ§Ã£o 3: Cron Job Local

Em um servidor Linux, adicione ao crontab:

```bash
# Run every Monday at 9:00 AM
0 9 * * 1 cd /path/to/travel-hr-buddy && npm run crawler:imca
```

## ğŸ”§ Troubleshooting

### Erro: Missing environment variables

Certifique-se de que as variÃ¡veis `VITE_SUPABASE_URL` e `SUPABASE_SERVICE_ROLE_KEY` estÃ£o definidas no `.env.local`.

### Erro: Cannot insert into dp_incidents

Verifique se a migraÃ§Ã£o `20251020000000_add_crawler_fields_to_dp_incidents.sql` foi aplicada:

```bash
# Se usando Supabase CLI local
supabase db push
```

### Nenhum incidente encontrado

O site da IMCA pode ter mudado sua estrutura HTML. Verifique os seletores CSS no arquivo `scripts/imca-crawler.ts`:

- `.news-list__item` - Container de cada incidente
- `.news-list__title` - TÃ­tulo do incidente
- `.news-list__date` - Data do incidente

## ğŸ“š DependÃªncias Instaladas

- `axios` - Para fazer requisiÃ§Ãµes HTTP
- `cheerio` - Para parsing de HTML (DOM parsing server-side)
- `tsx` - Para executar TypeScript diretamente
- `@supabase/supabase-js` - Cliente Supabase

## ğŸ” SeguranÃ§a

âš ï¸ **Nunca commite o arquivo `.env.local` no Git!**

O `SUPABASE_SERVICE_ROLE_KEY` tem acesso completo ao banco de dados, incluindo bypass de RLS. Mantenha-o seguro.

## âœ… PrÃ³ximos Passos

ApÃ³s executar o crawler, vocÃª pode:

1. âœ… Visualizar os incidentes no painel `/dp-intelligence`
2. âœ… Aplicar anÃ¡lise de IA nos incidentes (GPT-4)
3. âœ… Criar planos de aÃ§Ã£o automaticamente
4. âœ… Enviar alertas por e-mail para incidentes crÃ­ticos
5. âœ… Integrar com SGSO (Sistema de GestÃ£o de SeguranÃ§a Operacional)

---

**Desenvolvido para Travel HR Buddy - Sistema NÃ¡utico Inteligente** ğŸš¢âš“
